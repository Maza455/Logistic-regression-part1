{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2waJN_un3--Z"
   },
   "source": [
    "# Classification metrics assessment\n",
    "\n",
    "You can only use the packages provided\n",
    "* pandas\n",
    "* numpy\n",
    "* seaborn\n",
    "* matplotlib.pyplot\n",
    "* confusion_matrix from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "w9SJfgbF3--l",
    "outputId": "da799612-7310-42ac-ccea-43b8c4625241"
   },
   "source": [
    "## Using the data provided, calculate the following metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [7, 3, 2, 2, 2, 1,3,5,1,5,3]\n",
    "y_pred = [2, 2, 7, 7, 2, 2,3,5,4,5,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwhwMjKM3-_3"
   },
   "source": [
    "### Explain how these perfomance evaluation metrics are calculated and how they are used to evaluate the logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROC Curve explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 1 1\n"
     ]
    }
   ],
   "source": [
    "TPositives, FNegatives, FPositives, TNegatives = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "\n",
    "print(TPositives, FNegatives, FPositives, TNegatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specificity / True Negative Rate explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculations\n",
    "\n",
    "Specificity = TNegatives/(TNegatives + FPositives)\n",
    "\n",
    "print ('Specificity =', Specificity)\n",
    "\n",
    "# Intuitively speaking, if we have 100% specific model, that means it did NOT miss any True Negative, in other words, there were NO False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sensitivity/ recall/ True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculations\n",
    "\n",
    "Recall = TPositives/(TPositives + FNegatives)\n",
    "\n",
    "print ('Recall =', Recall)\n",
    "\n",
    "# Intuitively speaking, if we have a 100% sensitive model, that means it did NOT miss any True Positive, in other words, there were NO False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculations\n",
    "\n",
    "Precision = TPositives/(TPositives + FPositives)\n",
    "\n",
    "print ('Precision =', Precision)\n",
    "\n",
    "# Intuitively speaking, if we have a 100% precise model, that means it could catch all True positive but there were NO False Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate= 0.5\n"
     ]
    }
   ],
   "source": [
    "#Calculations\n",
    "\n",
    "FPositiveRate = (FPositives/(FPositives + TNegatives))\n",
    "\n",
    "print('False Positive Rate=', FPositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate= 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculations\n",
    "\n",
    "FNegativeRate = (FNegatives/(FNegatives + TPositives))\n",
    "\n",
    "print('False Negative Rate=', FNegativeRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recruit/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Calculations\n",
    "\n",
    "F1_score = 2*((Precision*Recall)/(Precision*Recall))\n",
    "\n",
    "print('F1 score =', F1_score)\n",
    "\n",
    "# F1 Score keeps a balance between Precision and Recall. We use it if there is uneven class distribution, as precision and recall may give misleading results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AUC score /balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score = 0.25\n"
     ]
    }
   ],
   "source": [
    "# Calculations\n",
    "\n",
    "AUC_score = (TPositives + TNegatives)/(TPositives + FPositives + FNegatives + TNegatives)\n",
    "\n",
    "print('AUC score =', AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'plot_roc_curve'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-71bbd774cc86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ROC Curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'plot_roc_curve'"
     ]
    }
   ],
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tjt3_dDv3-_4"
   },
   "source": [
    "# Useful references\n",
    "\n",
    "1. [Datacamp tutorial](https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "logistic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
